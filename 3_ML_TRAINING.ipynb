{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug-81i2VR7fO"
      },
      "source": [
        "# LLM Authorship Attribution (JavaScript) - 5-class Machine Learning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAag1t15OjLD"
      },
      "source": [
        "## STEP 0: Installing dependencies"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7vw1JrnTEgHy",
        "outputId": "8c631452-f007-47e9-def3-b342d2bf87e9"
      },
      "outputs": [],
      "source": [
        "# ============================ Imports ============================\n",
        "import pandas as pd\n",
        "import time\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "import json\n",
        "import warnings\n",
        "import joblib\n",
        "import math\n",
        "\n",
        "from operator import methodcaller\n",
        "from pathlib import Path\n",
        "\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "from sklearn.feature_extraction.text import TfidfVectorizer\n",
        "from sklearn.preprocessing import LabelEncoder, FunctionTransformer\n",
        "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
        "from sklearn.pipeline import Pipeline\n",
        "\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.svm import LinearSVC\n",
        "from xgboost import XGBClassifier\n",
        "\n",
        "warnings.filterwarnings(\"ignore\")\n",
        "\n",
        "# ============================ Step 1: Load Dataset ============================\n",
        "print(\"[*] Downloading dataset...\")\n",
        "!wget -O LLM-NodeJS-medium.json.zip https://github.com/LLM-NodeJS-dataset/LLM-NodeJS-dataset/releases/download/LLM-NodeJS-medium/LLM-NodeJS-medium.json.zip > /dev/null 2>&1\n",
        "!unzip -o LLM-NodeJS-medium.json.zip > /dev/null 2>&1\n",
        "!rm LLM-NodeJS-medium.json.zip > /dev/null 2>&1\n",
        "\n",
        "MAIN_DATASET_PATH = \"/content/LLM-NodeJS-medium.json\"\n",
        "CROSS_CHECK_DATASET_PATH = \"/content/CROSSCHECK-v2.json\"\n",
        "\n",
        "with open(MAIN_DATASET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "rows = data if isinstance(data, list) else [data]\n",
        "df = pd.json_normalize(rows, sep=\".\")\n",
        "print(\"[*] Training, Test Dataset loaded:\", df.shape)\n",
        "\n",
        "with open(CROSS_CHECK_DATASET_PATH, \"r\", encoding=\"utf-8\") as f:\n",
        "    data = json.load(f)\n",
        "rows = data if isinstance(data, list) else [data]\n",
        "crosscheck_df = pd.json_normalize(rows, sep=\".\")\n",
        "print(\"[*] Cross-Check Dataset loaded:\", crosscheck_df.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PEBEpPFFOyVX"
      },
      "source": [
        "## STEP 2: Machine Learning Training"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "6d9W0c3POhdG",
        "outputId": "290f9693-5865-4908-d5bb-9fb60c4304a3"
      },
      "outputs": [
        {
          "ename": "NameError",
          "evalue": "name 'df' is not defined",
          "output_type": "error",
          "traceback": [
            "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
            "\u001b[31mNameError\u001b[39m                                 Traceback (most recent call last)",
            "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[1]\u001b[39m\u001b[32m, line 244\u001b[39m\n\u001b[32m    238\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m all_results\n\u001b[32m    241\u001b[39m \u001b[38;5;66;03m# ---------------------------- Step 7: Main Execution ----------------------------\u001b[39;00m\n\u001b[32m    242\u001b[39m \n\u001b[32m    243\u001b[39m \u001b[38;5;66;03m# Keep only selected models\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m244\u001b[39m filtered_df = \u001b[43mdf\u001b[49m[df[\u001b[33m\"\u001b[39m\u001b[33mmodel_name\u001b[39m\u001b[33m\"\u001b[39m].isin(TARGET_MODELS)].copy()\n\u001b[32m    246\u001b[39m \u001b[38;5;66;03m# Encode labels\u001b[39;00m\n\u001b[32m    247\u001b[39m encoder = LabelEncoder()\n",
            "\u001b[31mNameError\u001b[39m: name 'df' is not defined"
          ]
        }
      ],
      "source": [
        "# -------------------------------- Configuration ---------------------------------\n",
        "\n",
        "# 20-class\n",
        "# TARGET_MODELS = [\n",
        "#     \"gpt-4o\", \"gpt-4o-mini\", \"gpt-5-mini\", \"gpt-5-nano\", \"gpt-oss-120b\",  # OpenAI\n",
        "#     \"gemini-2.0-flash\", \"gemini-2.5-flash-lite\", \"gemma-3-27b\", # Google\n",
        "#     \"llama-3.1-8b\", \"llama-3.3-70b\", \"llama-4-scout\",  # Meta\n",
        "#     \"codestral-2508\", \"mixtral-8x7b\",  # Mistral\n",
        "#     \"qwen-2.5-7b\", \"qwen-2.5-coder-32b\", \"qwen3-coder\",  # Alibaba\n",
        "#     \"grok-3-mini\", \"grok-code-fast-1\",  # xAI\n",
        "#     \"deepseek-v3.1\",  # DeepSeek\n",
        "#     \"phi-4-reasoning-plus\"  # Microsoft\n",
        "# ]\n",
        "\n",
        "# 10-class\n",
        "TARGET_MODELS = [\n",
        "    \"gpt-4o\",\"gpt-4o-mini\",\"gpt-5-mini\",\"gpt-5-nano\",\"gpt-oss-120b\",   # OpenAI\n",
        "    \"gemini-2.5-flash-lite\",  # Google\n",
        "    \"llama-3.3-70b\",  # Meta\n",
        "    \"mixtral-8x7b\",  # Mistral\n",
        "    \"qwen-2.5-coder-32b\",  # Alibaba\n",
        "    \"deepseek-v3.1\"  # DeepSeek\n",
        "]\n",
        "\n",
        "# 5-class\n",
        "# TARGET_MODELS = [\"gpt-4o\", \"gpt-4o-mini\" ,\"gpt-5-mini\", \"gpt-5-nano\", \"gpt-oss-120b\"]\n",
        "\n",
        "USED_DATASET_TYPES = [\"js_original\", \"terser_mangled\", \"js_deobfuscated\"]\n",
        "SAMPLE_SIZES = (12500,)\n",
        "\n",
        "\n",
        "# -------------------------- Step 1: Model Definitions ---------------------------\n",
        "\n",
        "def get_models():\n",
        "    return {\n",
        "        \"KNN\": KNeighborsClassifier(),\n",
        "        \"Random Forest\": RandomForestClassifier(n_estimators=400, random_state=42),\n",
        "        \"SVM (Linear)\": LinearSVC(max_iter=2000, random_state=42),\n",
        "        \"XGBoost\": XGBClassifier(n_estimators=400, max_depth=9, use_label_encoder=False, eval_metric=\"mlogloss\", random_state=42),\n",
        "        \"Logistic Regression\": LogisticRegression(max_iter=2000,random_state=42)\n",
        "    }\n",
        "\n",
        "\n",
        "# ---------------------------- Step 2: Data Splitting ----------------------------\n",
        "\n",
        "def get_dataframes(df, *, sample_size=None, test_size=0, val_size=0, random_state=42):\n",
        "    groups = df[\"prompt\"].unique()\n",
        "\n",
        "    current_df = df\n",
        "\n",
        "    if sample_size is not None and sample_size < len(df):\n",
        "        avg_rows_per_group = len(df) / len(groups)\n",
        "        n_groups_needed = int(sample_size / avg_rows_per_group)\n",
        "\n",
        "        if n_groups_needed < len(groups):\n",
        "            rng = np.random.RandomState(random_state)\n",
        "            selected_groups = rng.choice(groups, size=n_groups_needed, replace=False)\n",
        "\n",
        "            current_df = df[df[\"prompt\"].isin(selected_groups)].copy()\n",
        "        else:\n",
        "            print(f\"WARNING: The size of the Dataframe ({sample_size}) is smaller than the sample size ({len(df)})\")\n",
        "\n",
        "    if test_size > 0:\n",
        "        splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "        train_val_idx, test_idx = next(splitter.split(current_df, groups=current_df[\"prompt\"]))\n",
        "\n",
        "        train_val_df = current_df.iloc[train_val_idx]\n",
        "        test_df = current_df.iloc[test_idx].copy()\n",
        "    else:\n",
        "        train_val_df = current_df\n",
        "        test_df = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    if val_size > 0:\n",
        "        relative_val_size = val_size / (1 - test_size)\n",
        "        if relative_val_size >= 1.0:\n",
        "             raise ValueError(\"ERROR: The sum of test and val size reaches or exceeds 1.0!\")\n",
        "\n",
        "        splitter = GroupShuffleSplit(n_splits=1, test_size=relative_val_size, random_state=random_state)\n",
        "        train_idx, val_idx = next(splitter.split(train_val_df, groups=train_val_df[\"prompt\"]))\n",
        "\n",
        "        train_df = train_val_df.iloc[train_idx].copy()\n",
        "        val_df = train_val_df.iloc[val_idx].copy()\n",
        "    else:\n",
        "        train_df = train_val_df.copy()\n",
        "        val_df = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "# -------------------------- Step 3: Feature Extraction --------------------------\n",
        "\n",
        "def prepare_features(train_df, val_df, test_df, vectorizer):\n",
        "    X_train = vectorizer.fit_transform(train_df[\"js_code\"].fillna(\"\")).toarray()\n",
        "    X_val = vectorizer.transform(val_df[\"js_code\"].fillna(\"\")).toarray() if not val_df.empty else np.array([])\n",
        "    X_test = vectorizer.transform(test_df[\"js_code\"].fillna(\"\")).toarray()\n",
        "\n",
        "    y_train = train_df[\"label\"].to_numpy()\n",
        "    y_val = val_df[\"label\"].to_numpy() if not val_df.empty else np.array([])\n",
        "    y_test = test_df[\"label\"].to_numpy()\n",
        "\n",
        "    return X_train, y_train, X_val, y_val, X_test, y_test\n",
        "\n",
        "\n",
        "# ------------------------ Step 4: Training & Evaluation -------------------------\n",
        "\n",
        "def train_models(models, X_train, y_train):\n",
        "    metrics = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            start_time = time.time()\n",
        "            model.fit(X_train, y_train)\n",
        "            elapsed = time.time() - start_time\n",
        "\n",
        "            metrics[name] = {\n",
        "                \"train_time_sec\": elapsed\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: {name} failed: {e}\")\n",
        "\n",
        "    return metrics\n",
        "\n",
        "\n",
        "def evaluate_models(models, X_test, y_test):\n",
        "    metrics = {}\n",
        "\n",
        "    for name, model in models.items():\n",
        "        try:\n",
        "            y_pred = model.predict(X_test)\n",
        "            metrics[name] = {\n",
        "                \"accuracy\": accuracy_score(y_test, y_pred),\n",
        "                \"precision\": precision_score(y_test, y_pred, average=\"weighted\", zero_division=0),\n",
        "                \"recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
        "                \"f1_score\": f1_score(y_test, y_pred, average=\"weighted\")\n",
        "            }\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: {name} failed: {e}\")\n",
        "    \n",
        "    return metrics\n",
        "\n",
        "\n",
        "def save_models(vectorizer, encoder, models, sample_size, base_dir):\n",
        "    root_path = Path(base_dir)\n",
        "    size_path = root_path / f\"size_{sample_size}\"\n",
        "    size_path.mkdir(parents=True, exist_ok=True)\n",
        "\n",
        "    joblib.dump(encoder, root_path / \"label_encoder.joblib\")\n",
        "    to_dense_transformer = FunctionTransformer(methodcaller(\"toarray\"), accept_sparse=True)\n",
        "\n",
        "    for name, model in models.items():\n",
        "        pipeline = Pipeline([\n",
        "            (\"tfidf\", vectorizer),\n",
        "            (\"to_dense\", to_dense_transformer),\n",
        "            (\"classifier\", model)\n",
        "        ])\n",
        "        joblib.dump(pipeline, size_path / f\"pipeline_{name.replace(\" \", \"_\")}.joblib\")\n",
        "\n",
        "    print(f\"[*] Trained Models Saved\")\n",
        "\n",
        "\n",
        "# ---------------------------- Step 5: Visualization -----------------------------\n",
        "\n",
        "def plot_sample_size_results(all_results, metrics=(\"accuracy\", \"precision\", \"recall\", \"f1_score\")):\n",
        "    sample_sizes = sorted(all_results.keys())\n",
        "    models = list(next(iter(all_results.values())).keys())\n",
        "\n",
        "    n_metrics = len(metrics)\n",
        "    n_cols = 2\n",
        "    n_rows = math.ceil(n_metrics / n_cols)\n",
        "\n",
        "    fig, axes = plt.subplots(n_rows, n_cols, figsize=(14, 5 * n_rows))\n",
        "    \n",
        "    if n_metrics > 1:\n",
        "        axes = axes.flatten()\n",
        "    else:\n",
        "        axes = [axes]\n",
        "\n",
        "    for ax, metric in zip(axes, metrics):\n",
        "        for model in models:\n",
        "            values = [all_results[size][model][metric] for size in sample_sizes]\n",
        "            ax.plot(sample_sizes, values, marker=\"o\", label=model)\n",
        "\n",
        "        ax.set_xlabel(\"Sample Size\")\n",
        "        ax.set_ylabel(metric.title())\n",
        "        ax.set_ylim(0, 1.05)\n",
        "        ax.grid(True, linestyle=\"--\", alpha=0.6)\n",
        "\n",
        "    for i in range(n_metrics, len(axes)):\n",
        "        fig.delaxes(axes[i])\n",
        "\n",
        "    handles, labels = axes[0].get_legend_handles_labels()\n",
        "    fig.legend(handles, labels, loc='center right', bbox_to_anchor=(0.98, 0.5), fontsize='medium')\n",
        "\n",
        "    plt.tight_layout()\n",
        "    plt.subplots_adjust(right=0.85)\n",
        "    \n",
        "    plt.show()\n",
        "\n",
        "\n",
        "# -------------------------- Step 6: Training Pipeline ---------------------------\n",
        "\n",
        "def run_training_pipeline(df, *, encoder=None, save_models_dir=None, sample_sizes=(12500,)):\n",
        "    print(\"[*] Precomputing full feature matrix...\")\n",
        "    all_results = {}\n",
        "    for size in sample_sizes:\n",
        "        if size > len(df):\n",
        "            print(f\"Skipping size {size}, dataset too small ({len(df)})\")\n",
        "            continue\n",
        "\n",
        "        train_df, val_df, test_df = get_dataframes(df, sample_size=size, val_size=0, test_size=0.2)\n",
        "\n",
        "        vectorizer = TfidfVectorizer(max_features=400, token_pattern=r\"(?u)\\b\\w+\\b\")\n",
        "        X_train, y_train, _, _, X_test, y_test = prepare_features(train_df, val_df, test_df, vectorizer)\n",
        "\n",
        "        print(f\"\\n[*] Evaluating Sample Size: {size}, Actual Size: {len(train_df) + len(val_df) + len(test_df)}\")\n",
        "        models = get_models()\n",
        "        train_metrics = train_models(models, X_train, y_train)\n",
        "        eval_metrics = evaluate_models(models, X_test, y_test)\n",
        "\n",
        "        if save_models_dir and encoder:\n",
        "            save_models(vectorizer, encoder, models, sample_size=size, base_dir=save_models_dir)\n",
        "\n",
        "        metrics = {name: train_metrics.get(name, {}) | eval_metrics.get(name, {}) for name in models}\n",
        "        all_results[size] = metrics\n",
        "\n",
        "        print(\"\\n\" + f\" Results for Sample Size {size} \".center(80, \"-\"))\n",
        "        for model_name, scores in metrics.items():\n",
        "            acc = scores[\"accuracy\"]\n",
        "            prec = scores[\"precision\"]\n",
        "            rec = scores[\"recall\"]\n",
        "            f1 = scores[\"f1_score\"]\n",
        "            time_sec = scores[\"train_time_sec\"]\n",
        "            print(f\"{model_name:20s} | Acc: {acc:.4f} | Prec: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f} | Time: {time_sec:.4f}s\")\n",
        "\n",
        "    # Plot curves\n",
        "    plot_sample_size_results(all_results)\n",
        "\n",
        "    return all_results\n",
        "\n",
        "\n",
        "# ---------------------------- Step 7: Main Execution ----------------------------\n",
        "\n",
        "# Keep only selected models\n",
        "filtered_df = df[df[\"model_name\"].isin(TARGET_MODELS)].copy()\n",
        "\n",
        "# Encode labels\n",
        "encoder = LabelEncoder()\n",
        "filtered_df[\"label\"] = encoder.fit_transform(filtered_df[\"model_name\"])\n",
        "\n",
        "print(\"[*] Filtered Dataset Shape:\", filtered_df.shape)\n",
        "print(\"[*] Classes:\", dict(zip(encoder.classes_, encoder.transform(encoder.classes_))), end=\"\\n\\n\")\n",
        "\n",
        "all_test_metrics = {}\n",
        "for dataset_type in USED_DATASET_TYPES:\n",
        "    filtered_df[\"js_code\"] = filtered_df[f\"{dataset_type}.js_code\"]\n",
        "\n",
        "    print(f\" {dataset_type} Dataset Type \".center(80, \"=\"))\n",
        "    test_metrics = run_training_pipeline(filtered_df, encoder=encoder, save_models_dir=Path(\"trained_models\") / dataset_type, sample_sizes=SAMPLE_SIZES)\n",
        "    all_test_metrics[dataset_type] = test_metrics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 3: Model Loading & Final Testing\n",
        "\n",
        "The `CROSSCHECK-v2.json` file was generated using the *OpenAI API* instead of the *OpenRouter.ai* service. The code snippets were generated with entirely different prompts. There are only 4 classes in cross-check dataset."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "[*] Dataset loaded: (50000, 27)\n",
            "=========================== js_original Dataset Type ===========================\n",
            "\n",
            "------------------------ Results for sample size 12500 -------------------------\n",
            "Random Forest        | Acc: 0.8320 | Prec: 0.8360 | Recall: 0.8320 | F1: 0.8331\n",
            "KNN                  | Acc: 0.5560 | Prec: 0.5706 | Recall: 0.5560 | F1: 0.5548\n",
            "Logistic Regression  | Acc: 0.7840 | Prec: 0.7857 | Recall: 0.7840 | F1: 0.7831\n",
            "SVM (Linear)         | Acc: 0.7960 | Prec: 0.7961 | Recall: 0.7960 | F1: 0.7950\n",
            "XGBoost              | Acc: 0.8460 | Prec: 0.8490 | Recall: 0.8460 | F1: 0.8469\n",
            "\n",
            "\n",
            "========================= terser_mangled Dataset Type ==========================\n",
            "\n",
            "------------------------ Results for sample size 12500 -------------------------\n",
            "Random Forest        | Acc: 0.7940 | Prec: 0.7979 | Recall: 0.7940 | F1: 0.7945\n",
            "KNN                  | Acc: 0.5620 | Prec: 0.5624 | Recall: 0.5620 | F1: 0.5553\n",
            "Logistic Regression  | Acc: 0.7580 | Prec: 0.7582 | Recall: 0.7580 | F1: 0.7573\n",
            "SVM (Linear)         | Acc: 0.7700 | Prec: 0.7700 | Recall: 0.7700 | F1: 0.7685\n",
            "XGBoost              | Acc: 0.8180 | Prec: 0.8204 | Recall: 0.8180 | F1: 0.8181\n",
            "\n",
            "\n",
            "========================= js_deobfuscated Dataset Type =========================\n",
            "\n",
            "------------------------ Results for sample size 12500 -------------------------\n",
            "Random Forest        | Acc: 0.7720 | Prec: 0.7837 | Recall: 0.7720 | F1: 0.7705\n",
            "KNN                  | Acc: 0.5300 | Prec: 0.5267 | Recall: 0.5300 | F1: 0.5233\n",
            "Logistic Regression  | Acc: 0.6880 | Prec: 0.7003 | Recall: 0.6880 | F1: 0.6868\n",
            "SVM (Linear)         | Acc: 0.7000 | Prec: 0.7124 | Recall: 0.7000 | F1: 0.6984\n",
            "XGBoost              | Acc: 0.7620 | Prec: 0.7639 | Recall: 0.7620 | F1: 0.7587\n",
            "\n",
            "\n"
          ]
        }
      ],
      "source": [
        "def load_models(models_path):\n",
        "    models = {}\n",
        "    models_path = Path(models_path)\n",
        "\n",
        "    pipeline_files = list(models_path.glob(\"pipeline_*.joblib\"))\n",
        "    for file_path in pipeline_files:\n",
        "        try:\n",
        "            name = file_path.stem.replace(\"pipeline_\", \"\").replace(\"_\", \" \")\n",
        "            pipeline = joblib.load(file_path)\n",
        "            models[name] = pipeline\n",
        "        except Exception as e:\n",
        "            print(f\"ERROR: Failed to load {file_path.name}: {e}\")\n",
        "\n",
        "    return models\n",
        "\n",
        "\n",
        "def cross_check(df, base_dir):\n",
        "    root_dir = Path(base_dir)\n",
        "    X_test = df[\"js_code\"].fillna(\"\").astype(str)\n",
        "\n",
        "    encoder = joblib.load(root_dir / \"label_encoder.joblib\")\n",
        "    y_test = encoder.transform(df[\"model_name\"])\n",
        "\n",
        "    size_paths = sorted(Path(base_dir).glob(\"size_*\"), key=lambda p: int(p.name.split(\"_\")[1]))\n",
        "    \n",
        "    if not size_paths:\n",
        "        print(f\"ERROR: No 'size_*' directories found in {base_dir}\")\n",
        "        return\n",
        "\n",
        "    all_metrics = {}\n",
        "    for size_path in size_paths:\n",
        "        loaded_pipelines = load_models(size_path)\n",
        "        \n",
        "        if not loaded_pipelines:\n",
        "            continue\n",
        "\n",
        "        size = size_path.name.split(\"_\")[-1]\n",
        "        metrics = evaluate_models(loaded_pipelines, X_test, y_test)\n",
        "        print(\"\\n\" + f\" Results for sample size {size} \".center(80, \"-\"))\n",
        "        for model_name, scores in metrics.items():\n",
        "            acc = scores[\"accuracy\"]\n",
        "            prec = scores[\"precision\"]\n",
        "            rec = scores[\"recall\"]\n",
        "            f1 = scores[\"f1_score\"]\n",
        "            print(f\"{model_name:20s} | Acc: {acc:.4f} | Prec: {prec:.4f} | Recall: {rec:.4f} | F1: {f1:.4f}\")\n",
        "\n",
        "        all_metrics[size] = metrics\n",
        "    \n",
        "    return all_metrics\n",
        "\n",
        "\n",
        "if len(TARGET_MODELS) > 5:\n",
        "    print(\"WARNING: There are only 4 classes in cross-check dataset!\\n\")\n",
        "\n",
        "filtered_crosscheck_df = crosscheck_df[crosscheck_df[\"model_name\"].isin(TARGET_MODELS)].copy()\n",
        "base_models_folder = Path(\"trained_models\")\n",
        "\n",
        "# Evaluate models by dataset type\n",
        "all_final_test_metrics = {}\n",
        "for dataset_type in USED_DATASET_TYPES:\n",
        "    print(f\" {dataset_type} Dataset Type \".center(80, \"=\"))\n",
        "    filtered_crosscheck_df[\"js_code\"] = filtered_crosscheck_df[f\"{dataset_type}.js_code\"]\n",
        "\n",
        "    model_dir = base_models_folder / dataset_type\n",
        "    if model_dir.exists():\n",
        "        final_test_metrics = cross_check(filtered_crosscheck_df, base_dir=model_dir)\n",
        "        all_final_test_metrics[dataset_type] = final_test_metrics\n",
        "    else:\n",
        "        print(f\"Skipping {dataset_type}: directory not found.\")\n",
        "\n",
        "    print(end=\"\\n\\n\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## STEP 4: Divergence Analysis - Compare Final Test Results with Test Results\n",
        "\n",
        "There are only 4 classes in cross-check dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 85,
      "metadata": {},
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "===================================\n",
            "| PERFORMANCE DIVERGENCE ANALYSIS |\n",
            "===================================\n",
            " >> Dataset Type: JS_ORIGINAL\n",
            "\n",
            "   [Sample Size: 12500]\n",
            "   ------------------------------------------------------------------------------------------\n",
            "   Model                     | Acc (Int)  | Acc (Ext)  | Diff (Acc)   | Diff (F1)   \n",
            "   ------------------------------------------------------------------------------------------\n",
            "   KNN                       | 0.4976     | 0.5560     | +0.0584       | +0.0631\n",
            "   Random Forest             | 0.8255     | 0.8320     | +0.0065       | +0.0068\n",
            "   SVM (Linear)              | 0.7853     | 0.7960     | +0.0107       | +0.0096\n",
            "   XGBoost                   | 0.8502     | 0.8460     | -0.0042       | -0.0041\n",
            "   Logistic Regression       | 0.7765     | 0.7840     | +0.0075       | +0.0064\n",
            "\n",
            "\n",
            " >> Dataset Type: TERSER_MANGLED\n",
            "\n",
            "   [Sample Size: 12500]\n",
            "   ------------------------------------------------------------------------------------------\n",
            "   Model                     | Acc (Int)  | Acc (Ext)  | Diff (Acc)   | Diff (F1)   \n",
            "   ------------------------------------------------------------------------------------------\n",
            "   KNN                       | 0.4729     | 0.5620     | +0.0891       | +0.0947\n",
            "   Random Forest             | 0.7618     | 0.7940     | +0.0322       | +0.0339\n",
            "   SVM (Linear)              | 0.7295     | 0.7700     | +0.0405       | +0.0406\n",
            "   XGBoost                   | 0.8040     | 0.8180     | +0.0140       | +0.0142\n",
            "   Logistic Regression       | 0.7084     | 0.7580     | +0.0496       | +0.0507\n",
            "\n",
            "\n",
            " >> Dataset Type: JS_DEOBFUSCATED\n",
            "\n",
            "   [Sample Size: 12500]\n",
            "   ------------------------------------------------------------------------------------------\n",
            "   Model                     | Acc (Int)  | Acc (Ext)  | Diff (Acc)   | Diff (F1)   \n",
            "   ------------------------------------------------------------------------------------------\n",
            "   KNN                       | 0.4781     | 0.5300     | +0.0519       | +0.0573\n",
            "   Random Forest             | 0.7625     | 0.7720     | +0.0095       | +0.0092\n",
            "   SVM (Linear)              | 0.7259     | 0.7000     | -0.0259       | -0.0259\n",
            "   XGBoost                   | 0.7865     | 0.7620     | -0.0245       | -0.0275\n",
            "   Logistic Regression       | 0.7112     | 0.6880     | -0.0232       | -0.0223\n",
            "\n",
            "\n",
            "====================================================================================================\n",
            " AGGREGATED RESULTS (Average divergence between sets)\n",
            "====================================================================================================\n",
            " Average Accuracy Divergence: +0.0195  (Negative = Worse performance on Cross-Check)\n",
            " Average F1-Score Divergence: +0.0204\n",
            "====================================================================================================\n"
          ]
        }
      ],
      "source": [
        "def analyze_performance_divergence(internal_results, external_results_dict, dataset_types):\n",
        "    print(\"===================================\")\n",
        "    print(\"| PERFORMANCE DIVERGENCE ANALYSIS |\")\n",
        "    print(\"===================================\")\n",
        "\n",
        "    all_accuracy_diffs = []\n",
        "    all_f1_diffs = []\n",
        "\n",
        "    for dtype in dataset_types:\n",
        "        if dtype not in internal_results or dtype not in external_results_dict:\n",
        "            continue\n",
        "\n",
        "        print(f\" >> Dataset Type: {dtype.upper()}\")\n",
        "\n",
        "        int_data_by_size = internal_results[dtype]\n",
        "        ext_data_by_size = external_results_dict[dtype]\n",
        "\n",
        "        sorted_sizes = sorted(int_data_by_size.keys())\n",
        "        \n",
        "        for size in sorted_sizes:\n",
        "            if str(size) not in ext_data_by_size and size not in ext_data_by_size:\n",
        "                print(f\"   [!] Warning: No cross-check data for size {size}.\")\n",
        "                continue\n",
        "            \n",
        "            ext_models = ext_data_by_size.get(str(size)) or ext_data_by_size.get(size)\n",
        "            int_models = int_data_by_size[size]\n",
        "            \n",
        "            print(f\"\\n   [Sample Size: {size}]\")\n",
        "            print(f\"   {'-'*90}\")\n",
        "            print(f\"   {'Model':<25} | {'Acc (Int)':<10} | {'Acc (Ext)':<10} | {'Diff (Acc)':<12} | {'Diff (F1)':<12}\")\n",
        "            print(f\"   {'-'*90}\")\n",
        "\n",
        "            for model_name in int_models:\n",
        "                if model_name in ext_models:\n",
        "                    acc_int = int_models[model_name]['accuracy']\n",
        "                    acc_ext = ext_models[model_name]['accuracy']\n",
        "                    f1_int = int_models[model_name]['f1_score']\n",
        "                    f1_ext = ext_models[model_name]['f1_score']\n",
        "\n",
        "                    diff_acc = acc_ext - acc_int\n",
        "                    diff_f1 = f1_ext - f1_int\n",
        "\n",
        "                    all_accuracy_diffs.append(diff_acc)\n",
        "                    all_f1_diffs.append(diff_f1)\n",
        "\n",
        "                    sign = \"+\" if diff_acc >= 0 else \"\"\n",
        "                    \n",
        "                    print(f\"   {model_name:<25} | {acc_int:.4f}     | {acc_ext:.4f}     | {sign}{diff_acc:.4f}       | {diff_f1:+.4f}\")\n",
        "            print(\"\\n\")\n",
        "\n",
        "    if all_accuracy_diffs:\n",
        "        avg_acc_diff = sum(all_accuracy_diffs) / len(all_accuracy_diffs)\n",
        "        avg_f1_diff = sum(all_f1_diffs) / len(all_f1_diffs)\n",
        "\n",
        "        print(f\"{'='*100}\")\n",
        "        print(f\" AGGREGATED RESULTS (Average divergence between sets)\")\n",
        "        print(f\"{'='*100}\")\n",
        "        print(f\" Average Accuracy Divergence: {avg_acc_diff:+.4f}  (Negative = Worse performance on Cross-Check)\")\n",
        "        print(f\" Average F1-Score Divergence: {avg_f1_diff:+.4f}\")\n",
        "\n",
        "        print(f\"{'='*100}\")\n",
        "\n",
        "\n",
        "analyze_performance_divergence(all_test_metrics, all_final_test_metrics, USED_DATASET_TYPES)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "provenance": []
    },
    "kernelspec": {
      "display_name": ".venv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.14.0"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
