{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ug-81i2VR7fO"
      },
      "source": [
        "#LLM Authorship Attribution (JavaScript)\n",
        "\n",
        "This Google Colab contains the code associated with the paper **The Hidden DNA of LLM-Generated JavaScript: Structural Patterns Enable High-Accuracy Authorship Attribution**, accepted at the 3rd International Workshop on Large Language Models for Code (LLM4Code â€™26).\n",
        "\n",
        "**Description:** This script can be used to train the modified CodeT5 model for testing LLM authorship attribution. The Colab notebook provides all the necessary steps to train BERT to distinguish which LLM generated the JavaScript code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HAag1t15OjLD"
      },
      "source": [
        "##STEP 0: Installing dependencies  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JvQndCvG9Vn0",
        "outputId": "53199546-3bab-4101-d6d8-919887b6d562"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[*] Installing dependencies...\n",
            "[*] Installation: DONE\n",
            "[*] Loading: DONE\n",
            "[*] Downloading LLM-NodeJS Medium dataset\n",
            "[*] Dataset downloaded: DONE\n",
            "[*] Dataset processed: DONE\n",
            "[*] GPU in available: NVIDIA A100-SXM4-40GB\n",
            "[*] Available models in dataset: [codestral-2508, deepseek-v3.1, gemini-2.0-flash, gemini-2.5-flash-lite, gemma-3-27b, gpt-4o, gpt-4o-mini, gpt-5-mini, gpt-5-nano, gpt-oss-120b, grok-3-mini, grok-code-fast-1, llama-3.1-8b, llama-3.3-70b, llama-4-scout, mixtral-8x7b, phi-4-reasoning-plus, qwen-2.5-7b, qwen-2.5-coder-32b, qwen3-coder]\n"
          ]
        }
      ],
      "source": [
        "#STEP 0: Install dependencies, download the dataset, prepare helper functions, and import all required packages\n",
        "import os\n",
        "# Install dependencies\n",
        "print(\"[*] Installing dependencies...\")\n",
        "!pip install --upgrade transformers > /dev/null 2>&1\n",
        "print(\"[*] Installation: DONE\")\n",
        "\n",
        "# Load packages\n",
        "import torch, json, tempfile, shutil, zipfile, datetime\n",
        "import pandas as pd\n",
        "from tqdm import tqdm\n",
        "from sklearn.metrics import confusion_matrix\n",
        "import seaborn as sns\n",
        "import matplotlib.pyplot as plt\n",
        "from sklearn.model_selection import GroupShuffleSplit\n",
        "from sklearn.metrics import accuracy_score, precision_recall_fscore_support\n",
        "from datasets import Dataset\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import numpy as np\n",
        "from transformers import Trainer, TrainingArguments, DataCollatorWithPadding\n",
        "print(\"[*] Loading: DONE\")\n",
        "\n",
        "# Download dataset\n",
        "print(\"[*] Downloading LLM-NodeJS Medium dataset\")\n",
        "!wget -O LLM-NodeJS-medium.json.zip https://github.com/LLM-NodeJS-dataset/LLM-NodeJS-dataset/releases/download/LLM-NodeJS-medium/LLM-NodeJS-medium.json.zip > /dev/null 2>&1\n",
        "!unzip -o LLM-NodeJS-medium.json.zip > /dev/null 2>&1\n",
        "!rm LLM-NodeJS-medium.json.zip > /dev/null 2>&1\n",
        "print(\"[*] Dataset downloaded: DONE\")\n",
        "\n",
        "# Load dataset\n",
        "PATH = '/content/LLM-NodeJS-medium.json'\n",
        "with open(PATH, 'r', encoding='utf-8') as f:\n",
        "    data = json.load(f)\n",
        "rows = data if isinstance(data, list) else [data]\n",
        "df = pd.json_normalize(rows, sep='.')\n",
        "models = df['model_name'].dropna().astype(str).unique().tolist()\n",
        "print(\"[*] Dataset processed: DONE\")\n",
        "\n",
        "# Check if CUDA (GPU) is available\n",
        "if torch.cuda.is_available():\n",
        "    device_name = torch.cuda.get_device_name(0)\n",
        "    device_count = torch.cuda.device_count()\n",
        "    print(f\"[*] GPU in available: {device_name}\")\n",
        "    os.environ.setdefault(\"PYTORCH_CUDA_ALLOC_CONF\", \"expandable_segments:True\")\n",
        "    if torch.cuda.is_available():\n",
        "        torch.backends.cuda.matmul.fp32_precision = \"tf32\"\n",
        "        torch.backends.cudnn.conv.fp32_precision = \"tf32\"\n",
        "else:\n",
        "    print(\"[*] CUDA is NOT available. Using CPU only.\")\n",
        "\n",
        "print(f\"[*] Available models in dataset: [{', '.join(models)}]\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "7vw1JrnTEgHy"
      },
      "outputs": [],
      "source": [
        "# ------------------------------------------------------------------------------------------------------------------------------------\n",
        "# CodeT5-specific helpers\n",
        "# ------------------------------------------------------------------------------------------------------------------------------------\n",
        "import transformers\n",
        "from torch.nn import CrossEntropyLoss\n",
        "from transformers.modeling_outputs import SequenceClassifierOutput\n",
        "\n",
        "\n",
        "def load_tokenizer():\n",
        "    return transformers.AutoTokenizer.from_pretrained(\"Salesforce/codet5p-770m\")\n",
        "\n",
        "def load_model():\n",
        "    print(f'Loading CodeT5 model without decocder layers...')\n",
        "    model_kwargs = dict(dtype=torch.bfloat16)\n",
        "    transformers.T5EncoderModel._keys_to_ignore_on_load_unexpected = [\"decoder.*\"]\n",
        "    print(\"Modael loaded.\")\n",
        "    encoder = transformers.T5EncoderModel.from_pretrained(\"Salesforce/codet5p-770m\", **model_kwargs)\n",
        "    return encoder\n",
        "\n",
        "class CodeT5_Classifier(torch.nn.Module):\n",
        "    def __init__(self, pretrained_encoder, hidden_dim, num_labels=20):\n",
        "        super().__init__()\n",
        "        self.encoder = pretrained_encoder\n",
        "        self.config = pretrained_encoder.config\n",
        "        self.config.num_labels = num_labels\n",
        "\n",
        "        self.pre_classifier = torch.nn.Linear(hidden_dim, 768, dtype=torch.bfloat16)\n",
        "        self.activation = torch.nn.Tanh()\n",
        "        self.dropout = torch.nn.Dropout(0.2)\n",
        "        self.classifier = torch.nn.Linear(768, num_labels, dtype=torch.bfloat16)\n",
        "        self.loss_fn = CrossEntropyLoss()\n",
        "\n",
        "    def forward(self, input_ids=None, attention_mask=None, labels=None, **kwargs):\n",
        "        outputs = self.encoder(input_ids=input_ids, attention_mask=attention_mask)\n",
        "        hidden_state = outputs.last_hidden_state if hasattr(outputs, \"last_hidden_state\") else outputs[0]\n",
        "\n",
        "        cls_output = hidden_state[:, 0]\n",
        "        cls_output = self.dropout(cls_output)\n",
        "        pooled = self.pre_classifier(cls_output)\n",
        "        pooled = self.activation(pooled)\n",
        "        pooled = self.dropout(pooled)\n",
        "        logits = self.classifier(pooled)\n",
        "\n",
        "        loss = None\n",
        "        if labels is not None:\n",
        "            loss = self.loss_fn(logits, labels)\n",
        "\n",
        "        return SequenceClassifierOutput(loss=loss, logits=logits)\n",
        "\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Training helpers (save, metrics, visualization)\n",
        "# ------------------------------------------------------------------------------------------------------------------------------------\n",
        "\n",
        "\n",
        "def COMPUTE_metrics(pred):\n",
        "    labels = pred.label_ids\n",
        "    preds = pred.predictions.argmax(-1)\n",
        "    p, r, f1, _ = precision_recall_fscore_support(labels, preds, average=\"weighted\")\n",
        "    acc = accuracy_score(labels, preds)\n",
        "    return {\"accuracy\": acc, \"f1\": f1, \"precision\": p, \"recall\": r}\n",
        "\n",
        "def SAVE_confusion_matrix(trainer, test_ds, label_encoder, out_path=\"confusion_matrix.pdf\"):\n",
        "    predictions = trainer.predict(test_ds)\n",
        "    y_true = predictions.label_ids\n",
        "    y_pred = predictions.predictions.argmax(axis=1)\n",
        "    conf_matrix = confusion_matrix(y_true, y_pred)\n",
        "\n",
        "    plt.figure(figsize=(8, 6))\n",
        "    sns.heatmap(conf_matrix, annot=True, fmt=\"d\", cmap=\"Blues\",\n",
        "                xticklabels=label_encoder.classes_,\n",
        "                yticklabels=label_encoder.classes_)\n",
        "    plt.xlabel(\"Predicted\")\n",
        "    plt.ylabel(\"Actual\")\n",
        "    plt.title(\"Confusion Matrix\")\n",
        "    plt.tight_layout()\n",
        "    plt.savefig(out_path)\n",
        "    plt.show()\n",
        "    plt.close()\n",
        "\n",
        "    print(f\"[*] Confusion matrix saved to: {out_path}\")\n",
        "\n",
        "def get_dataframes(df, *, sample_size=None, test_size=0, val_size=0, random_state=42):\n",
        "    groups = df[\"prompt\"].unique()\n",
        "\n",
        "    current_df = df\n",
        "\n",
        "    if sample_size is not None and sample_size < len(df):\n",
        "        avg_rows_per_group = len(df) / len(groups)\n",
        "        n_groups_needed = int(sample_size / avg_rows_per_group)\n",
        "\n",
        "        if n_groups_needed < len(groups):\n",
        "            rng = np.random.RandomState(random_state)\n",
        "            selected_groups = rng.choice(groups, size=n_groups_needed, replace=False)\n",
        "\n",
        "            current_df = df[df[\"prompt\"].isin(selected_groups)].copy()\n",
        "        else:\n",
        "            print(f\"WARNING: The size of the Dataframe ({sample_size}) is smaller than the sample size ({len(df)})\")\n",
        "\n",
        "    if test_size > 0:\n",
        "        splitter = GroupShuffleSplit(n_splits=1, test_size=test_size, random_state=random_state)\n",
        "        train_val_idx, test_idx = next(splitter.split(current_df, groups=current_df[\"prompt\"]))\n",
        "\n",
        "        train_val_df = current_df.iloc[train_val_idx]\n",
        "        test_df = current_df.iloc[test_idx].copy()\n",
        "    else:\n",
        "        train_val_df = current_df\n",
        "        test_df = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    if val_size > 0:\n",
        "        relative_val_size = val_size / (1 - test_size)\n",
        "        if relative_val_size >= 1.0:\n",
        "             raise ValueError(\"ERROR: The sum of test and val size reaches or exceeds 1.0!\")\n",
        "\n",
        "        splitter = GroupShuffleSplit(n_splits=1, test_size=relative_val_size, random_state=random_state)\n",
        "        train_idx, val_idx = next(splitter.split(train_val_df, groups=train_val_df[\"prompt\"]))\n",
        "\n",
        "        train_df = train_val_df.iloc[train_idx].copy()\n",
        "        val_df = train_val_df.iloc[val_idx].copy()\n",
        "    else:\n",
        "        train_df = train_val_df.copy()\n",
        "        val_df = pd.DataFrame(columns=df.columns)\n",
        "\n",
        "    return train_df, val_df, test_df\n",
        "\n",
        "\n",
        "# ------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Fine-tuning pipeline for CodeT5\n",
        "# ------------------------------------------------------------------------------------------------------------------------------------\n",
        "def RUN_fine_tuning(INPUT_TEXT, TARGET_COLUMN, MAX_TOKEN, CLASSES, df):\n",
        "    filtered_df = df[df[TARGET_COLUMN].isin(CLASSES)].copy()\n",
        "    encoder = LabelEncoder()\n",
        "    filtered_df[\"labels\"] = encoder.fit_transform(filtered_df[TARGET_COLUMN])\n",
        "    num_labels = len(encoder.classes_)\n",
        "    print(\"Number of labels:\", num_labels)\n",
        "\n",
        "    train_df, _, test_df = get_dataframes(filtered_df, val_size=0, test_size=0.2)\n",
        "\n",
        "    tokenizer = load_tokenizer()\n",
        "    encoder_model = load_model()\n",
        "    model = CodeT5_Classifier(encoder_model, hidden_dim=encoder_model.shared.embedding_dim, num_labels=num_labels)\n",
        "\n",
        "    train_ds = Dataset.from_pandas(train_df)\n",
        "    test_ds = Dataset.from_pandas(test_df)\n",
        "\n",
        "    def tokenize_function(batch):\n",
        "        return tokenizer(batch[INPUT_TEXT], truncation=True, max_length=MAX_TOKEN, add_special_tokens=True)\n",
        "\n",
        "    train_ds = train_ds.map(tokenize_function, batched=True, num_proc=4, desc=\"Tokenizing train set\")\n",
        "    test_ds = test_ds.map(tokenize_function, batched=True, num_proc=4, desc=\"Tokenizing test set\")\n",
        "\n",
        "    train_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "    test_ds.set_format(\"torch\", columns=[\"input_ids\", \"attention_mask\", \"labels\"])\n",
        "\n",
        "    data_collator = DataCollatorWithPadding(tokenizer=tokenizer, pad_to_multiple_of=8)\n",
        "\n",
        "    training_args = TrainingArguments(\n",
        "        output_dir=\"./results\",\n",
        "        num_train_epochs=10,\n",
        "        per_device_train_batch_size=8,\n",
        "        per_device_eval_batch_size=16,\n",
        "        gradient_accumulation_steps=2,\n",
        "        eval_accumulation_steps=16,\n",
        "        bf16=True, fp16=False,\n",
        "        optim=\"adamw_torch_fused\",\n",
        "        learning_rate=2e-5,\n",
        "        warmup_ratio=0.06,\n",
        "        lr_scheduler_type=\"cosine_with_restarts\",\n",
        "        weight_decay=0.01,\n",
        "        eval_strategy=\"epoch\",\n",
        "        save_strategy=\"epoch\",\n",
        "        logging_strategy=\"epoch\",\n",
        "        load_best_model_at_end=True,\n",
        "        label_smoothing_factor=0.05,\n",
        "        max_grad_norm=1.0,\n",
        "        metric_for_best_model=\"accuracy\",\n",
        "        save_total_limit=2,\n",
        "        logging_dir=\"./logs\",\n",
        "        logging_steps=100,\n",
        "        report_to=\"none\",\n",
        "        dataloader_num_workers=2,\n",
        "        dataloader_pin_memory=True,\n",
        "        save_safetensors=False\n",
        "    )\n",
        "\n",
        "\n",
        "\n",
        "    trainer = Trainer(\n",
        "        model=model,\n",
        "        args=training_args,\n",
        "        data_collator=data_collator,\n",
        "        train_dataset=train_ds,\n",
        "        eval_dataset=test_ds,\n",
        "        compute_metrics=COMPUTE_metrics,\n",
        "    )\n",
        "\n",
        "    trainer.train()\n",
        "    trainer.evaluate()\n",
        "\n",
        "    return trainer, encoder, tokenizer, test_ds\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jiILD_Od9ylk"
      },
      "outputs": [],
      "source": [
        "\n",
        "# ------------------------------------------------------------------------------------------------------------------------------------\n",
        "# Run training (CodeT5)\n",
        "# ------------------------------------------------------------------------------------------------------------------------------------\n",
        "INPUT_TEXT = \"js_original.js_code\"\n",
        "TARGET_COLUMN = \"model_name\"\n",
        "MAX_TOKEN = 512\n",
        "\n",
        "\n",
        "# 5-class\n",
        "TARGET_MODELS = [\"gpt-4o\", \"gpt-4o-mini\" ,\"gpt-5-mini\", \"gpt-5-nano\", \"gpt-oss-120b\"]\n",
        "\n",
        "# ------- MODEL config parameters ----------------\n",
        "CLASS_num_lables=str(len(TARGET_MODELS))+\"-class_family_\"\n",
        "\n",
        "print(f\"\\n===== Training CodeT5 (Salesforce/codet5p-770m) =====\")\n",
        "trainer, encoder, tokenizer, test_ds = RUN_fine_tuning(INPUT_TEXT, TARGET_COLUMN, MAX_TOKEN, TARGET_MODELS, df)\n",
        "\n",
        "cm_path = \"CodeT5_confusion_matrix.pdf\"\n",
        "SAVE_confusion_matrix(trainer, test_ds, encoder, out_path=cm_path)\n",
        "\n",
        "\n",
        "print(\"===== Finished CodeT5 Training =====\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "A100",
      "machine_shape": "hm",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
